---
title: "Single Cell Sample Size Calculation"
output: rmarkdown::html_vignette
author: Yingxin Lin
vignette: >
  %\VignetteIndexEntry{sampleSizeCal}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

This vignette will provide an example showing how to fit the learning curve based on classification accuracy of `scClassify`.

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  comment = "#>"
)
```




Suppose we have a small pilot reference data, and we would like to use this reference data to build scClassify model. We can construct the learning curve to estimate the number of cells required in a reference dataset to discriminate accurately between two cell types at a given level in the cell type hierarchy.

# Data

Here, we illustrate the learning curve construction using a small example data with 4 cell types, with only 980 genes and 674 cells.


```{r setup}
library(scClassify)


load("data/scClassify_example.rda")
xin_cellTypes <- scClassify_example$xin_cellTypes
exprsMat_xin_subset <- scClassify_example$exprsMat_xin_subset

exprsMat_xin_subset <- as(exprsMat_xin_subset, "dgCMatrix")

dim(exprsMat_xin_subset)
table(xin_cellTypes)
```

We can first have a look at the cell type tree constructed by the whole data. We can see here, we have a cell type tree that have two levels cell type labels:

+ **First level**: "gamma_delta_beta", "alpha"
+ **Second level**:  "gamma", "delta", "beta", "alpha"



```{r}
load("data/trainClassExample.rda")
trainClassExample
plotCellTypeTree(trainClassExample@cellTypeTree)
```

By default, `scClassify` will construct the learning curve based on the bottom level (i.e., second level in the example).

# Learning curve construction

Next, we can get an accuracy matrix by running `runSampleCal` function, where the parameter `n_list` indicates a list of sample size (N) that you would like to subsample from the data as training set, and `num_repeat` indicates the number of times to repeat this procedure for each `N`.

For the purpose of illustration, here we set a short length of `n_list` and a small number of `num_repeat`. A longer `n_list` and a larger number of `num_repeat` is recommended for better learning curve fitting.

```{r eval = TRUE, results='hide'}
set.seed(2019)
system.time(accMat <- runSampleCal(exprsMat_xin_subset, 
                                   xin_cellTypes,
                                   n_list = seq(40, 200, 40), 
                                   num_repeat = 5, ncores = 1))
```


After we obtain an accuracy matrix, we can fit the learning curve using `learningCurve` function.

```{r eval = TRUE, results='hide'}
res <- learningCurve(accMat = accMat,
                     n = as.numeric(colnames(accMat)))
```

Check the parameters estimated from average accuracy rates.

```{r}
res$model$mean
```


```{r fig.width = 8, fig.height = 8}
res$plot
```




We can also get the sample size for future experiment given the accuracy rate using `getN` function

```{r}
getN(res, acc = 0.93)
```



# Constructing learning curve based on higher level of labels in cell type tree

We can input `cellType_tree` and specify the level of labels we would like to construct the learning curve. Here, we set `level = 1` to calculate the learning curve for **First level** cell type labels.


```{r eval = TRUE, results='hide'}
set.seed(2019)
system.time(accMat_level1 <- runSampleCal(exprsMat_xin_subset, 
                                          xin_cellTypes,
                                          n_list = seq(40, 200, 40), 
                                          num_repeat = 5,
                                          cellType_tree = trainClassExample@cellTypeTree,
                                          level = 1,
                                          ncores = 1))

res_level1 <- learningCurve(accMat_level1,
                            as.numeric(colnames(accMat_level1)))
```



```{r fig.width = 8, fig.height = 8}
res_level1$plot
```




# Session Info

```{r}
sessionInfo()
```
